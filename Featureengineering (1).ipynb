{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Assignment Questions**\n"
      ],
      "metadata": {
        "id": "bSDBTFlVtT3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is a parameter?**\n",
        "\n",
        "  A parameter is a value that describes a characteristic of a population or a model. It's a fixed, often unknown number that summarizes some aspect of the entire population or system you're studying.\n",
        "\n",
        "**2.What is correlation?**\n",
        "\n",
        "  Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "**What does negative correlation mean?**\n",
        "\n",
        "  A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, the two variables move in opposite directions.\n",
        "\n",
        "**3.Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "  Machine Learning is a branch of artificial intelligence (AI) that enables computers to learn patterns and make decisions or predictions from data without being explicitly programmed for every task. Instead of hard-coding rules, ML algorithms learn from examples to improve their performance on tasks over time.\n",
        "  Main components of Machine Learning\n",
        "  1.Data-: The foundational element. Includes input features (independent variables) and target/output (dependent variable).\n",
        "  2.Quality and quantity of data strongly affect model performance- Model is a  mathematical or computational representation of the process or relationship.\n",
        "  Examples: linear regression, decision trees, neural networks.\n",
        "  3.Algorithm-: The procedure or method used to train the model by learning patterns from data.\n",
        "  Examples: gradient descent, random forest training, backpropagation.\n",
        "  4.Training-: The process of feeding data into the model to learn parameters or rules. The model adjusts itself to minimize errors on training data.\n",
        "  5.Evaluation-: Assessing how well the trained model performs on new, unseen data. Metrics include accuracy, precision, recall, RMSE, etc.\n",
        "  6.Prediction-: Using the trained model to make predictions or decisions on new data.\n",
        "\n",
        "**4.How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "  The loss value plays a central role in training and evaluating machine learning models. It tells you how far off your model's predictions are from the actual values. he loss is a single number that represents the error made by the model on a single training example or an entire batch. It's calculated using a loss function (e.g., Mean Squared Error, Cross-Entropy Loss). A lower loss value means better model performance during training.\n",
        "\n",
        "**5.What are continuous and categorical variables?**\n",
        "\n",
        "  A continuous variable is a numeric variable that can take any value within a range — including decimals or fractions.\n",
        "  A categorical variable represents groups or categories. These values are qualitative, not numerical (even if they appear as numbers).\n",
        "\n",
        "**6.How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "  Machine learning models (like linear regression, SVMs, or neural networks) can't interpret text or category labels directly. You need to convert them into numbers while preserving the meaning.\n",
        "  Common techniques to handle categorical variables:\n",
        "  1.Label Encoding\n",
        "  2.One-Hot Encoding\n",
        "  3.Ordinal Encoding\n",
        "  4.Frequency or Count Encoding\n",
        "  5.Target Encoding (Mean Encoding)\n",
        "  6.Binary Encoding / Hashing (Advanced)\n",
        "\n",
        "**7.What do you mean by training and testing a dataset?**\n",
        "\n",
        "  Training a dataset means using it to teach the model how to make predictions. The model looks at both the input features (X) and the target output (y) to learn patterns or relationships. Testing means evaluating the model's performance on unseen data to check how well it generalizes.\n",
        "\n",
        "**8.What is sklearn.preprocessing?**\n",
        "\n",
        "  sklearn.preprocessing is a module in the Scikit-learn (sklearn) library that provides tools to prepare your data before training a machine learning model.\n",
        "  it helps you transform raw data into a format that models can understand and perform well on.\n",
        "\n",
        "**9.What is a Test set?**\n",
        "\n",
        "  A test set is a portion of your dataset that is kept separate and only used to evaluate the performance of your trained model.\n",
        "\n",
        "**10.How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "  In Python, especially with Scikit-learn, the most common way to split data into training and testing sets is by using the train_test_split function.\n",
        "  Splitting Data for Model Fitting\n",
        "   1.Import Required Modules\n",
        "   2.Assume You Have a Dataset\n",
        "   3.Split the Data\n",
        "   4.Use the Splits for Model Training\n",
        "\n",
        "**How do you approach a Machine Learning problem?**\n",
        "\n",
        "  Approaching a machine learning (ML) problem requires a structured process to ensure you're building a model that solves the right problem effectively and reliably.\n",
        "  1.Understand the Problem\n",
        "  2.Collect and Explore the Data\n",
        "  3.Preprocess the Data\n",
        "  4.Split the Data\n",
        "  5.Choose a Model\n",
        "  6.Train the Model\n",
        "  7.Evaluate the Model\n",
        "  8.Tune Hyperparameters\n",
        "  9.Repeat & Optimize\n",
        "  10.Deploy the Model\n",
        "\n",
        "**11.Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "  EDA (Exploratory Data Analysis) is the process of analyzing, visualizing, and summarizing the dataset to understand its structure, detect issues, and discover patterns before modeling. Perform EDA Before Model Fitting because\n",
        "  1.Understand Data Characteristics\n",
        "  2.Detect and Handle Missing Values\n",
        "  3.Identify Outliers\n",
        "  4.Reveal Data Distributions\n",
        "  5.Uncover Feature Relationships\n",
        "  6.Spot Data Imbalance\n",
        "  7.Guide Feature Engineering\n",
        "  8.Avoid Wasting Time on Poor Models\n",
        "\n",
        "**12.What is correlation?**\n",
        "\n",
        "  Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "**13.What does negative correlation mean?**\n",
        "\n",
        "  Negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "\n",
        "**14.How can you find correlation between variables in Python?**\n",
        "  \n",
        "  Finding correlation between variables in Python is straightforward, especially using libraries like pandas and seaborn.\n",
        "  1.Using pandas corr() method\n",
        "  2.Using Seaborn to visualize correlation matrix (heatmap)\n",
        "  3.Calculating correlation between two specific variables\n",
        "  4.Different correlation methods\n",
        "\n",
        "**15.What is causation? Explain difference between correlation and causation with an example.**\n",
        "\n",
        "  Causation (or cause-and-effect) means that one event is the direct result of another. In other words, changes in one variable directly cause changes in another variable.\n",
        "  Correlation-: Measures how two variables move together. Symmetric (A correlated with B means B correlated with A). Implies\tAssociation or relationship. Proof needed\tno, just statistical evidence.\n",
        "  Causation-: One variable directly influences the other. Asymmetric (A causes B, but B may not cause A). Implies\tCause and effect. Proof needed to controlled experiments or strong evidence\n",
        "\n",
        "**16.What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "  An optimizer is an algorithm or method used to adjust the parameters (like weights and biases) of a machine learning model to minimize the loss function (error) during training. It helps the model learn by finding the best set of parameters that reduce prediction errors.\n",
        "  Types of Optimizers and Explanation\n",
        "   1.Gradient Descent (GD)-: The basic optimizer.\n",
        "   Updates parameters by moving them in the direction of the negative gradient of the loss function.\n",
        "   Uses the entire training dataset to compute the gradient at every step.\n",
        "   2.Stochastic Gradient Descent (SGD)-: Instead of full dataset, updates parameters using one training example at a time. Faster updates but noisy (can jump around minima). Helps escape local minima.\n",
        "   3.Mini-batch Gradient Descent-:Compromise between GD and SGD. Uses a small batch of samples (e.g., 32, 64) to compute the gradient. Balances stability and speed.\n",
        "   4.Momentum-: Accelerates SGD by adding a fraction of the previous update to the current update. Helps avoid oscillations and speeds convergence.\n",
        "   5.Adagrad-: Adapts learning rate for each parameter based on historical gradients. Parameters with frequently large gradients get smaller updates.Good for sparse data.\n",
        "   6.RMSprop-: Improves Adagrad by using a moving average of squared gradients.\n",
        "   Prevents learning rate from shrinking too much.\n",
        "   7.Adam (Adaptive Moment Estimation)-: Combines Momentum and RMSprop. Maintains running averages of both gradients and squared gradients. Most popular optimizer due to fast convergence and good.\n",
        "\n",
        "**17.What is sklearn.linear_model ?**\n",
        "\n",
        "  sklearn.linear_model is a module in the Scikit-learn library that provides linear models for regression and classification tasks. It contains various algorithms based on linear relationships between input features and the target variable.\n",
        "\n",
        "**18.What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "  The .fit() method in machine learning is used to train a model on data.\n",
        "  Required Arguments for .fit(). Arguments\n",
        "  X- Feature matrix (2D array or DataFrame),shape = `(n_samples, n_features).\n",
        "  y- Target values (1D array, Series, or list),shape = (n_samples,)\n",
        "\n",
        "**19.What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "  model.predict() is a method used in machine learning models to make predictions on new data after the model has been trained. Takes input features (X) and returns the model's predicted output.\n",
        "  For classification, it predicts the class labels (e.g., \"spam\" or \"not spam\").\n",
        "  For regression, it predicts a continuous value (e.g., house price).\n",
        "  Arguments: The primary (and usually only) argument is the input features (X).\n",
        "  This must be in the same shape and format as the data the model was trained on\n",
        "\n",
        "**20.What are continuous and categorical variables?**\n",
        "\n",
        "  In data science and statistics, variables are often divided into two main types: continuous and categorical. This classification is important because different algorithms and preprocessing techniques are used for each type.\n",
        "  Continuous Variables-: Variables that can take any numerical value within a range, including decimals.\n",
        "  Categorical Variables-: Variables that represent categories, groups, or labels, often stored as strings or integers but without mathematical meaning\n",
        "\n",
        "**21.What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "  Feature scaling is the process of transforming numerical features so that they are on a similar scale — typically in a range like 0 to 1 or -1 to 1. It is an essential preprocessing step in many machine learning pipelines. Helps-:\n",
        "  1.KNN / K-Means\tuses distance between points. Without scaling, larger features dominate.\n",
        "  2.SVM / Logistic Regression\tuses gradient descent or dot products. Unscaled features slow or misguide optimization.\n",
        "  3.Neural Networks\tScaling leads to faster convergence and better stability during training.\n",
        "  4.Tree-based models does not need scaling, because they split on thresholds, not distance.\n",
        "\n",
        "**22.How do we perform scaling in Python?**\n",
        "\n",
        "  Python makes feature scaling easy using the scikit-learn library. Here's how you can do it step by step.\n",
        "  1.Standardization (Z-score Scaling)- Transforms data to have mean = 0 and standard deviation = 1\n",
        "  2.Min-Max Scaling-: Scales features to a fixed range, usually 0 to 1.\n",
        "  3.Robust Scaling (for data with outliers)-: Uses median and interquartile range (IQR) instead of mean/std.\n",
        "  4.MaxAbs Scaling-: Scales by the maximum absolute value, good for sparse data\n",
        "\n",
        "\n",
        "**23.What is sklearn.preprocessing?**\n",
        "\n",
        "  sklearn.preprocessing is a module in the scikit-learn library that provides a variety of tools to prepare your data for machine learning models. Machine learning models work best when input data is clean, consistent, and numeric — sklearn.preprocessing helps you transform raw data into a suitable format.\n",
        "\n",
        "**24.How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "  To train a machine learning model properly, you must split your dataset into at least:\n",
        "  Training data: Used to train the model.\n",
        "  Testing data: Used to evaluate how well the model performs on unseen data\n",
        "\n",
        "**25.Explain data encoding?**\n",
        "\n",
        "  Data encoding is the process of converting categorical (non-numeric) data into a numerical format that machine learning algorithms can understand. Most models (like logistic regression, SVM, KNN, neural networks) can't work directly with text or categories — they need numbers. That's where encoding comes in.\n"
      ],
      "metadata": {
        "id": "yjVozn92tTXU"
      }
    }
  ]
}